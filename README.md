# Fine_Tuning_LLM
code for fine-tuning bert, OpenAI's GPT (Generative Pre-trained Transformer) models for a specific language-related task. Fine-tuning allows the model to specialize in tasks such as Named Entity Recognition (NER), sentiment analysis, or other natural language processing (NLP) tasks.


LLMS:
Llama 3 70B - 8k Context Length
Llama 3 8B - 8k Context Length
Gemma 7B - 8k Context Length
Whisper Large V3 - open-source automatic speech recognition (ASR) system

Mixtral 8x7B - 32k Context length
Gemini 1.0 -  a new Mixture-of-Experts (MoE) architecture - 32k Context length 
GPT 4 Turbo - 128k Context Length window 
Claude 2.1 - 200k Context Length Window
Gemini 1.5 Pro - a new Mixture-of-Experts (MoE) architecture - 128,000 token context window till 1M Token Context Window
